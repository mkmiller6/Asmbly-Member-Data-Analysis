{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll pull in the raw Discourse (forum) usage data and convert it to a pandas df.\n",
    "\n",
    "The Discourse data is pulled via SQL using the Discourse Data Explorer plugin. Exported data is\n",
    "limited to 10,000 rows per query, so we'll pull in each csv and concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_list = []\n",
    "for i in range(1,7):\n",
    "    df = pd.read_csv(f\"raw-interval-user-stats-{i}.csv\")\n",
    "    df[\"visited_at\"] = pd.to_datetime(df[\"visited_at\"])\n",
    "    concat_list.append(df)\n",
    "\n",
    "discourse_df = pd.concat(concat_list, ignore_index=True, sort=False).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll do the same for the raw Skedda (equipment usage) data.\n",
    "\n",
    "Skedda allows data exports to CSVs for intervals up to 1 year, so our final dataframe will \n",
    "concatenate all of the available years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "skedda_ranges = [\n",
    "    \"feb2020_to_feb2021\",\n",
    "    \"feb2021_to_feb2022\",\n",
    "    \"feb2022_to_feb2023\",\n",
    "    \"feb2023_to_feb2024\",\n",
    "    \"feb2024_to_feb2025\",\n",
    "]\n",
    "skedda_concats = []\n",
    "for date_range in skedda_ranges:\n",
    "    df = pd.read_csv(f\"skedda_bookings_{date_range}.csv\")\n",
    "    df[\"Scheduled start\"] = pd.to_datetime(df[\"Scheduled start\"])\n",
    "    df[\"End\"] = pd.to_datetime(df[\"End\"])\n",
    "    skedda_concats.append(df)\n",
    "\n",
    "skedda_df = pd.concat(skedda_concats, ignore_index=True, sort=False).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll pull in the Neon member data we've collected via get_asmbly_member_data_long_form.py\n",
    "\n",
    "For each user, we'll find the Discourse usage stats and Skedda usage stats for each period of\n",
    "membership (if any). We'll also calculate the change in usage from period-to-period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"all_members_long_form.csv\")\n",
    "\n",
    "base_discourse_cols = [\"discourse_read_time\", \"discourse_posts_read\", \"discourse_posts_made\"]\n",
    "\n",
    "user_df_dict = {}\n",
    "for row in df.itertuples():\n",
    "    if (user_df := user_df_dict.get(row.discourse_id)) is None:\n",
    "        user_df = discourse_df[discourse_df.username == row.discourse_id]\n",
    "        user_df_dict[row.discourse_id] = user_df\n",
    "\n",
    "    time_restricted_df = user_df[(user_df.visited_at <= row.end_date) & (user_df.visited_at >= row.start_date)]\n",
    "\n",
    "    for col in base_discourse_cols:\n",
    "        split = col.split(\"_\")\n",
    "        new = split[2] + \"_\" + split[1] if \"time\" in split else split[1] + \"_\" + split[2]\n",
    "        df.loc[row.Index, col] = time_restricted_df[new].sum()\n",
    "\n",
    "    try:\n",
    "        prev_id = df.loc[row.Index-1, \"discourse_id\"]\n",
    "    except KeyError:\n",
    "        for col in base_discourse_cols:\n",
    "            df.loc[row.Index, f\"change_{col}\"] = df.loc[row.Index, col]\n",
    "        continue\n",
    "\n",
    "    for col in base_discourse_cols:\n",
    "        df.loc[row.Index, f\"change_{col}\"] = (df.loc[row.Index, col] - df.loc[(row.Index - 1), col]) if prev_id == row.discourse_id else df.loc[row.Index, col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_dict = {}\n",
    "for row in df.itertuples():\n",
    "    if (user_df := user_df_dict.get(row.email)) is None:\n",
    "        user_df = skedda_df[skedda_df[\"Holder email\"] == row.email]\n",
    "        user_df_dict[row.email] = user_df\n",
    "\n",
    "    time_restricted_df = user_df[(user_df[\"Scheduled start\"] <= row.end_date) & (user_df[\"Scheduled start\"] >= row.start_date)]\n",
    "\n",
    "    df.loc[row.Index, \"skedda_booking_count\"] = time_restricted_df[\"Duration (minutes)\"].count()\n",
    "    df.loc[row.Index, \"skedda_booking_minutes\"] = time_restricted_df[\"Duration (minutes)\"].sum()\n",
    "\n",
    "    try:\n",
    "        prev_email = df.loc[row.Index-1, \"email\"]\n",
    "    except KeyError:\n",
    "        df.loc[row.Index, \"change_skedda_booking_count\"] = df.loc[row.Index, \"skedda_booking_count\"]\n",
    "        df.loc[row.Index, \"change_skedda_booking_minutes\"] = df.loc[row.Index, \"skedda_booking_minutes\"]\n",
    "        continue\n",
    "\n",
    "    df.loc[row.Index, \"change_skedda_booking_count\"] = (df.loc[row.Index, \"skedda_booking_count\"] - df.loc[(row.Index - 1), \"skedda_booking_count\"]) if prev_email == row.email else df.loc[row.Index, \"skedda_booking_count\"]\n",
    "    df.loc[row.Index, \"change_skedda_booking_minutes\"] = (df.loc[row.Index, \"skedda_booking_minutes\"] - df.loc[(row.Index - 1), \"skedda_booking_minutes\"]) if prev_email == row.email else df.loc[row.Index, \"skedda_booking_minutes\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, one additional variable type that could be relevant in our time-varying survival analysis is the \n",
    "cumulative sum of time-varying variables. This is in essence a measure of the \"sunk cost\" for the member over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_var_cols = [\n",
    "    \"discourse_read_time\",\n",
    "    \"discourse_posts_made\",\n",
    "    \"discourse_posts_read\",\n",
    "    \"skedda_booking_count\",\n",
    "    \"skedda_booking_minutes\",\n",
    "    \"num_classes_attended\",\n",
    "    \"dollars_spent\",\n",
    "    \"woodshop_classes\",\n",
    "    \"metal_shop_classes\",\n",
    "    \"electronics_classes\",\n",
    "    \"textiles_classes\",\n",
    "    \"lasers_classes\",\n",
    "    \"3dp_classes\"\n",
    "]\n",
    "\n",
    "for row in df.itertuples():\n",
    "\n",
    "    try:\n",
    "        prev_id = df.loc[row.Index-1, \"neon_id\"]\n",
    "    except KeyError:\n",
    "        for col in time_var_cols:\n",
    "            df.loc[row.Index, f\"cum_{col}\"] = df.loc[row.Index, col]\n",
    "        continue\n",
    "\n",
    "    for col in time_var_cols:\n",
    "        df.loc[row.Index, f\"cum_{col}\"] = (df.loc[row.Index, col] + df.loc[(row.Index - 1), f\"cum_{col}\"]) if prev_id == row.neon_id else df.loc[row.Index, col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's deal with the remaining missing values in the age, gender, distance and time from Asmbly,\n",
    "and referral source columns. Depending on the model used, the model itself may be able to handle \n",
    "the missing data without issue (i.e. ensemble tree models). However, for now we'll assume we're \n",
    "going to use a time-varying Cox Proportional Hazards model, where we need to handle missing values.\n",
    "\n",
    "Accounts without a referral source were all created before we began tracking this metric. We can \n",
    "assume that these old accounts found out about Asmbly through their own searching (i.e. Google), \n",
    "since Google was one of the only means of finding Asmbly back then. This also happens to coincide\n",
    "with the mode of referral source.\n",
    "\n",
    "There are a relatively small number of accounts missing distance/time data due to malformed addresses. \n",
    "For these, we'll just use the median values.\n",
    "\n",
    "For age and gender, we could use the FiveThirtyEight approach of imputation based on name, but there\n",
    "are a number of issues with prediciting age and a binary gender based on name alone. Because of this, \n",
    "we will simply use the median and mode, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_med = df[\"distance_from_asmbly\"].median()\n",
    "time_med = df[\"time_from_asmbly\"].median()\n",
    "\n",
    "referral_mode = df[\"referral_source\"].mode()[0]\n",
    "\n",
    "age_med = df[\"age\"].median()\n",
    "gender_mode = df[\"gender\"].mode()[0]\n",
    "\n",
    "df[\"distance_from_asmbly\"].fillna(distance_med, inplace=True)\n",
    "df[\"time_from_asmbly\"].fillna(time_med, inplace=True)\n",
    "df[\"referral_source\"].fillna(referral_mode, inplace=True)\n",
    "df[\"age\"].fillna(age_med, inplace=True)\n",
    "df[\"gender\"].fillna(gender_mode, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer need a few of these columns now that we've joined all the data, so let's drop them.\n",
    "\n",
    "We'll also rename a few of the columns to be a bit more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"email\",\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "    \"discourse_id\",\n",
    "    \"first_name\",\n",
    "    \"last_name\",\n",
    "]\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "rename = {\n",
    "    \"woodshop_safety\": \"taken_WSS_class\",\n",
    "    \"lasers\": \"taken_lasers_class\",\n",
    "    \"cnc_router\": \"taken_cnc_class\",\n",
    "    \"num_classes_attended\": \"num_classes_attended_last_period\",\n",
    "    \"metal_shop_safety\": \"taken_MSS_class\",\n",
    "}\n",
    "\n",
    "df.rename(columns=rename, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll convert out annual membership start and stop periods to the same unit (months) as \n",
    "the monthly memberships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.membership_type == \"YEAR\", [\"start\", \"stop\"]] *= 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all_members_long_form_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1247"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"neon_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AsmblyMakerspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
